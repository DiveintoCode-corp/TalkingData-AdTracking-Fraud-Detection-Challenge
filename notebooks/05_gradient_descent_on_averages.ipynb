{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to use previous learning materials to create predictions on the test dataset using an SGDClassifier.\n",
    "\n",
    "We will use the prediction mechanism demonstrated in **04_making_your_first_predictions_and_submission.ipynb** to create estimations of the target based on several features.\n",
    "\n",
    "At the end of this notebook you will know how to :\n",
    " - use SGDClassifier **partial_fit()** method to update a classifier with newly received samples\n",
    " - manage several target averages using a python **dict**\n",
    " - create test predictions in a gzip format\n",
    " \n",
    "A public kaggle kernel has been created to help you submit the predictions to the LeaderBoard:\n",
    "\n",
    "https://www.kaggle.com/ogrellier/gradient-descent-on-averages\n",
    "\n",
    "This will make it simpler for you to fork the script and use it for your own experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import time\n",
    "import gc\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the file_path so that it points to where the train file is on your system  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../input/train.csv.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify data types to limit memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "        'ip': 'uint32',\n",
    "        'app': 'uint16',\n",
    "        'device': 'uint16',\n",
    "        'os': 'uint16',\n",
    "        'channel': 'uint16',\n",
    "        'is_attributed': 'uint8'\n",
    "    }\n",
    "cols = [f_ for f_ in dtypes.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a average manager class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageManager(object):\n",
    "    \"\"\" Class that will manage target averages for selected feature and target encode these features \"\"\"\n",
    "    def __init__(self, features, target):\n",
    "        \"\"\" Init an average manager for the given features and the specified target \"\"\"\n",
    "        # averages contains the average data\n",
    "        self.averages = {\n",
    "            f: None for f in features\n",
    "        }\n",
    "        # Prior contains the estimated prior of the target \n",
    "        self.prior = {'cum_sum': 0.0, 'nb_samples': 0.0}\n",
    "        # Conatins the name of the target column in the DataFrames\n",
    "        self.target = target\n",
    "        \n",
    "    def update_averages(self, df):\n",
    "        \"\"\"Update averages information using samples available in df\"\"\"\n",
    "        # update prior\n",
    "        self.prior['cum_sum'] += df[self.target].sum()\n",
    "        self.prior['nb_samples'] += df.shape[0]\n",
    "        \n",
    "        for f_ in self.averages.keys():\n",
    "            # Create the groupby\n",
    "            the_group = df[[f_, self.target]].groupby(f_).agg(['sum', 'count'])\n",
    "            the_group.columns = the_group.columns.droplevel(0)\n",
    "    \n",
    "            # Update the average\n",
    "            if self.averages[f_] is None:\n",
    "                self.averages[f_] = the_group\n",
    "            else:\n",
    "                # pandas .add method makes sure apps that are not in both the_group and current averages\n",
    "                # take value of 0 before the addition takes place\n",
    "                self.averages[f_] = the_group.add(self.averages[f_], fill_value=0.0)\n",
    "            \n",
    "            del the_group\n",
    "            gc.collect()\n",
    "            \n",
    "    def apply_averages(self, df):\n",
    "        \"\"\"Apply calculated averages on df to target encode the features\"\"\"\n",
    "        encoded = pd.DataFrame()\n",
    "        for f_ in self.averages.keys():\n",
    "            if self.averages[f_] is None:\n",
    "                raise ValueError('Averages have not been fitted yet')\n",
    "            self.averages[f_]['average'] = self.averages[f_]['sum'] / self.averages[f_]['count']\n",
    "            encoded[f_] = df[f_].map(self.averages[f_]['average']).astype(np.float32)\n",
    "            prior = self.prior['cum_sum'] / self.prior['nb_samples']\n",
    "            encoded[f_].fillna(prior, inplace=True)\n",
    "        \n",
    "        return encoded\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read train dataset, update averages and train SGDClassifier\n",
    "\n",
    "Averages are updated on the fly and used to target encode the features one chunk after the other.\n",
    "\n",
    "SGDClassifier is trained using partial_fit.\n",
    "\n",
    "Doing things in this way will limit using future samples target on previous data like we would do when calating averages on the whole training set. Overall averages would use events that occured on the last day of the training set to encode events that occured on the 1st day of the training set.\n",
    "\n",
    "This is something we need to take care of when predicting on time series. \n",
    "\n",
    "Although I am not totally sure it is important in the TalkingData situation, I believe it would still limit overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk   1 scores : loss 0.012570 auc 0.945788 [  0.6 min used so far]\n",
      "Chunk   2 scores : loss 0.015511 auc 0.950864 [  1.2 min used so far]\n",
      "Chunk   3 scores : loss 0.014772 auc 0.963106 [  1.8 min used so far]\n",
      "Chunk   4 scores : loss 0.013282 auc 0.940642 [  2.4 min used so far]\n",
      "Chunk   5 scores : loss 0.016336 auc 0.957017 [  3.1 min used so far]\n",
      "Chunk   6 scores : loss 0.013681 auc 0.961123 [  3.7 min used so far]\n",
      "Chunk   7 scores : loss 0.012856 auc 0.937371 [  4.3 min used so far]\n",
      "Chunk   8 scores : loss 0.016054 auc 0.941511 [  4.9 min used so far]\n",
      "Chunk   9 scores : loss 0.016195 auc 0.951270 [  5.6 min used so far]\n",
      "Chunk  10 scores : loss 0.012138 auc 0.966746 [  5.7 min used so far]\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "# Create average manager\n",
    "used_features = ['app', 'os']\n",
    "avg_man = AverageManager(features=used_features, target='is_attributed')\n",
    "\n",
    "# Init Classifier\n",
    "clf = SGDClassifier(loss='log', tol=1e-2)\n",
    "\n",
    "# Read train file \n",
    "chunksize=20000000\n",
    "for i_chunk, df in enumerate(pd.read_csv(file_path, \n",
    "                                         chunksize=chunksize, \n",
    "                                         dtype=dtypes, \n",
    "                                         usecols=used_features + ['is_attributed'])):\n",
    "    # Udpate averages with the average manager\n",
    "    avg_man.update_averages(df)\n",
    "    \n",
    "    # Apply averages usin the average manager\n",
    "    target_encoding = avg_man.apply_averages(df)\n",
    "    \n",
    "    # Update the SGDClassifier using current target encoding and calling partial_fit\n",
    "    clf.partial_fit(X=target_encoding, y=df['is_attributed'], classes=[0, 1])\n",
    "    \n",
    "    # Get current predictions\n",
    "    preds = clf.predict_proba(target_encoding)[:, 1]\n",
    "    \n",
    "    # Display the log_loss and AUC score on the current chunk\n",
    "    print(\"Chunk %3d scores : loss %.6f auc %.6f [%5.1f min used so far]\"\n",
    "          % (i_chunk + 1, \n",
    "             log_loss(df['is_attributed'], preds),\n",
    "             roc_auc_score(df['is_attributed'], preds),\n",
    "             (time.time() - start_time) / 60))\n",
    "    \n",
    "    del target_encoding\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Chunks have been read in   0.1 minute\n",
      "  2 Chunks have been read in   0.2 minute\n",
      "  3 Chunks have been read in   0.2 minute\n",
      "  4 Chunks have been read in   0.3 minute\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "# Create place holder for the prediction\n",
    "predictions = None\n",
    "# PLEASE CHANGE THE TEST FILE PATH TO YOUR OWN SETTINGS\n",
    "test_file_path = '../input/test.csv.zip'\n",
    "chunksize = 5000000\n",
    "# Read the test file by chunks\n",
    "for i_chunk, df in enumerate(pd.read_csv(test_file_path, \n",
    "                                         chunksize=chunksize, \n",
    "                                         dtype=dtypes, \n",
    "                                         usecols=used_features + ['click_id'])):\n",
    "    if predictions is None:\n",
    "        # Get the click ids\n",
    "        # double square brackets are used to return a DataFrame and not a Series\n",
    "        predictions = df[['click_id']].copy() \n",
    "        # Encode df using average manager\n",
    "        target_encoding = avg_man.apply_averages(df)\n",
    "        # Predict probabilities with SGD Classifier\n",
    "        predictions['is_attributed'] = clf.predict_proba(target_encoding)[:, 1]\n",
    "    else:\n",
    "        # double square brackets are used to return a DataFrame and not a Series\n",
    "        curr_preds = df[['click_id']].copy() \n",
    "        # Encode df using average manager\n",
    "        target_encoding = avg_man.apply_averages(df)\n",
    "        # Predict probabilities with SGD Classifier\n",
    "        curr_preds['is_attributed'] = clf.predict_proba(target_encoding)[:, 1]\n",
    "        # Stack predictions and current predictions\n",
    "        predictions: pd.DataFrame = pd.concat([predictions, curr_preds], axis=0)\n",
    "        # free memory\n",
    "        del curr_preds\n",
    "        \n",
    "    # Free memory by deleting the current DataFrame\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Display the time we spent so far\n",
    "    print(\"%3d Chunks have been read in %5.1f minute\" \n",
    "          % (i_chunk + 1, (time.time() - start_time) / 60))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our predictions we need to store them in a file for submission.\n",
    "\n",
    "In this contest the submission file is quite big. To reduce its size, both for storage and submission over the web,we will use the following arguments:\n",
    " - float_format : it is used to cut the decimals of floats\n",
    " - compression : pandas can store files in a compressed format called gzip\n",
    " \n",
    "Writing this file can take some time! On my disk the file takes 108778KB.\n",
    "\n",
    "In the following statement:\n",
    " - **float_format** limits the number of decimal to 6\n",
    " - **compression** tells pandas to compress the file in gzip format\n",
    " - **index=False** tells pandas only to write the features themeselves in the file without the DataFrame index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('app_predictions.csv.gz', float_format='%.6f', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to avoid compressing the file or sending it through the web, which may also take some time, the best is to log into your Kaggle account and create kernel. You can then submit the result directly to the Leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Please use the previous code to create test predictions with more features :\n",
    " - ip\n",
    " - device\n",
    " - channel\n",
    " \n",
    "And give us your results and let us know if it helps you in progressing through the LeaderBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
