{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate the use of **defaultdict** to build statistics.\n",
    "\n",
    "**defaultdict** is a high performance version of python **dict**.\n",
    "\n",
    "You can read more [here](https://docs.python.org/2/library/collections.html) \n",
    "\n",
    "In particular, **defaultdict** can create keys on the fly, when you would have to check the existence of a key prior to using it in a standard python **dict** object\n",
    "\n",
    "At the end of this notebook, you will know how to :\n",
    " - go through the rows of a DataFrame using an iterator\n",
    " - create a **defaultdict**, create new keys on the fly and simple statistics\n",
    " - create predictions using the **map** method of pd.Series objects\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the file_path so that it points to where the train file is on your system  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../input/train.csv.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the hard work you have done in the first notebook you can define the best data types for each columns.\n",
    "\n",
    "Again in this notebook we will exclude all time related columns. \n",
    "\n",
    "Here are the data types definition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "        'ip': 'uint32',\n",
    "        'app': 'uint16',\n",
    "        'device': 'uint16',\n",
    "        'os': 'uint16',\n",
    "        'channel': 'uint16',\n",
    "        'is_attributed': 'uint8'\n",
    "    }\n",
    "cols = [f_ for f_ in dtypes.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read data by chunks will use the chunksize argument of pd.read_csv method \n",
    "\n",
    "chunksize is the maximum number of rows each chunk should be made of.\n",
    "\n",
    "pandas will read the file and give access the the first N rows then the following N rows and so on until the end of the file.\n",
    "\n",
    "We will use **defaultdict** from the python collections package to compute simple averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Chunks have been read in   0.1 minute\n",
      "  2 Chunks have been read in   0.5 minute\n",
      "  3 Chunks have been read in   1.0 minute\n",
      "  4 Chunks have been read in   1.4 minute\n",
      "  5 Chunks have been read in   1.8 minute\n",
      "  6 Chunks have been read in   2.2 minute\n",
      "  7 Chunks have been read in   2.6 minute\n",
      "  8 Chunks have been read in   3.1 minute\n",
      "  9 Chunks have been read in   3.5 minute\n",
      " 10 Chunks have been read in   4.0 minute\n",
      " 11 Chunks have been read in   4.4 minute\n",
      " 12 Chunks have been read in   4.9 minute\n",
      " 13 Chunks have been read in   5.3 minute\n",
      " 14 Chunks have been read in   5.7 minute\n",
      " 15 Chunks have been read in   6.2 minute\n",
      " 16 Chunks have been read in   6.6 minute\n",
      " 17 Chunks have been read in   7.1 minute\n",
      " 18 Chunks have been read in   7.5 minute\n",
      " 19 Chunks have been read in   7.8 minute\n"
     ]
    }
   ],
   "source": [
    "# chunksize is the maximum number of rows each chunk should be made of\n",
    "import time\n",
    "import gc\n",
    "# Enable garbage collection\n",
    "gc.enable()\n",
    "\n",
    "# Please adapt the chunksize to your memory setup\n",
    "chunksize = 10000000\n",
    "\n",
    "# Import defaultdict\n",
    "from collections import defaultdict\n",
    "\n",
    "def update_dicts(row, sum_dict, count_dict):\n",
    "    # row[0] is the ip address\n",
    "    # row[1] is is_attributed\n",
    "    sum_dict[row[0]] += row[1]\n",
    "    count_dict[row[0]] += 1 \n",
    "    \n",
    "# Create defaultdicts for sum and count\n",
    "ip_attributed = defaultdict(float)\n",
    "ip_count = defaultdict(float)\n",
    "\n",
    "start_time = time.time()\n",
    "for i_chunk, df in enumerate(pd.read_csv(file_path, chunksize=chunksize, dtype=dtypes, usecols=cols)):\n",
    "    print(\"%3d Chunks have been read in %5.1f minute\" \n",
    "          % (i_chunk + 1, (time.time() - start_time) / 60))\n",
    "    # Go through the rows of the current DataFrame chunk\n",
    "    # Please note that this is a lot quicker than using the .apply method\n",
    "    for ip, attributed in df[['ip', \"is_attributed\"]].values:\n",
    "            ip_attributed[ip] += attributed\n",
    "            ip_count[ip] += 1\n",
    "    \n",
    "    # Free memory by deleting the current DataFrame\n",
    "    del df\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this is a lot longer than the **groupby** method we used in the previous notebook. However you can do more complicated tasks with this simple method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in train :  277396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of keys in train : \", len(ip_attributed.keys()))\n",
    "for key in ip_attributed.keys():\n",
    "    ip_attributed[key] /= ip_count[key]\n",
    "    \n",
    "del ip_count\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Chunks have been processed in   0.1 minute\n",
      "  2 Chunks have been processed in   0.2 minute\n",
      "  3 Chunks have been processed in   0.4 minute\n",
      "  4 Chunks have been processed in   0.5 minute\n",
      "  5 Chunks have been processed in   0.6 minute\n",
      "  6 Chunks have been processed in   0.8 minute\n",
      "  7 Chunks have been processed in   0.9 minute\n",
      "  8 Chunks have been processed in   1.0 minute\n",
      "  9 Chunks have been processed in   1.2 minute\n",
      " 10 Chunks have been processed in   1.3 minute\n",
      " 11 Chunks have been processed in   1.4 minute\n",
      " 12 Chunks have been processed in   1.6 minute\n",
      " 13 Chunks have been processed in   1.7 minute\n",
      " 14 Chunks have been processed in   1.8 minute\n",
      " 15 Chunks have been processed in   2.0 minute\n",
      " 16 Chunks have been processed in   2.1 minute\n",
      " 17 Chunks have been processed in   2.3 minute\n",
      " 18 Chunks have been processed in   2.4 minute\n",
      " 19 Chunks have been processed in   2.5 minute\n"
     ]
    }
   ],
   "source": [
    "# Create ip_average as an empty DataFrame\n",
    "start_time=time.time()\n",
    "# Create place holders for target and predictions to be able to compute the AUC score once the process has completed\n",
    "target = None\n",
    "predictions = None \n",
    "for i_chunk, df in enumerate(pd.read_csv(file_path, chunksize=chunksize, dtype=dtypes, usecols=['ip', 'is_attributed'])):\n",
    "    print(\"%3d Chunks have been processed in %5.1f minute\" \n",
    "          % (i_chunk + 1, (time.time() - start_time) / 60))\n",
    "    if target is None:\n",
    "        target = df['is_attributed'].values\n",
    "        predictions = df['ip'].map(dict(ip_attributed)).values\n",
    "    else:\n",
    "        target = np.hstack((target, df['is_attributed'].values))\n",
    "        predictions = np.hstack((predictions, df['ip'].map(dict(ip_attributed))))\n",
    "        \n",
    "    # Free memory by deleting the current DataFrame\n",
    "    del df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display AUC score for this simple prediction on training dataset\n",
    "\n",
    "Please note this may take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score of predictions using ip on the whole dataset = 0.825532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"AUC score of predictions using ip on the whole dataset = %.6f\"\n",
    "      % (roc_auc_score(target, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
