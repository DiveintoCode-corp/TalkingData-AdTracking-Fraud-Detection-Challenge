{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to show how you can read a very big file chunk by chunk.\n",
    "\n",
    "In Talking Data competition, if you load train.csv file with a standard pd.read_csv command, the resulting DataFrame would use 11GB of memory. This is too large for most of personal computers setup but you can still read and work with whole data using pandas. \n",
    "\n",
    "By the end of this session you will know how to :\n",
    " \n",
    " - read a csv file in chunks using the **chunksize** argument, i.e. in slices of N rows\n",
    " - iterate though the chunks\n",
    " - use DataFrame **groupby** method to create simple average statistics\n",
    " - use **pd.DataFrame.add** to sum dataframes with different index's entries\n",
    " - use average statistics to make simple predictions\n",
    " \n",
    "So let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the file_path so that it points to where the train file is on your system  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../input/train.csv.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the hard work you have done in the first notebook you can define the best data types for each columns.\n",
    "\n",
    "Again in this notebook we will exclude all time related columns. \n",
    "\n",
    "Here are the data types definition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "        'ip': 'uint32',\n",
    "        'app': 'uint16',\n",
    "        'device': 'uint16',\n",
    "        'os': 'uint16',\n",
    "        'channel': 'uint16',\n",
    "        'is_attributed': 'uint8'\n",
    "    }\n",
    "cols = [f_ for f_ in dtypes.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read data by chunks will use the chunksize argument of pd.read_csv method \n",
    "\n",
    "chunksize is the maximum number of rows each chunk should be made of.\n",
    "\n",
    "pandas will read the file and give the 1st N rows then the following N rows and so on until the end of the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Chunks have been read in   0.1 minute\n",
      "  2 Chunks have been read in   0.2 minute\n",
      "  3 Chunks have been read in   0.3 minute\n"
     ]
    }
   ],
   "source": [
    "# chunksize is the maximum number of rows each chunk should be made of\n",
    "import time\n",
    "import gc\n",
    "# Enable garbage collection\n",
    "gc.enable()\n",
    "chunksize = 10000000\n",
    "start_time = time.time()\n",
    "for i_chunk, df in enumerate(pd.read_csv(file_path, chunksize=chunksize, dtype=dtypes, usecols=cols)):\n",
    "    print(\"%3d Chunks have been read in %5.1f minute\" \n",
    "          % (i_chunk + 1, (time.time() - start_time) / 60))\n",
    "    # Free memory by deleting the current DataFrame\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Make sure we stop after 3 chunks\n",
    "    if i_chunk >= 2 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if you read by chunks you can still limit the number of total rows you want to access using nrows like in the following example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Chunks have been read in   0.1 minute\n",
      "  2 Chunks have been read in   0.2 minute\n",
      "  3 Chunks have been read in   0.3 minute\n"
     ]
    }
   ],
   "source": [
    "chunksize = 10000000\n",
    "start_time = time.time()\n",
    "# Read by chunks and limit the total number of rows accessed to the first 30 000 000 rows\n",
    "for i_chunk, df in enumerate(pd.read_csv(file_path, chunksize=chunksize, dtype=dtypes, usecols=cols, nrows=30000000)):\n",
    "    print(\"%3d Chunks have been read in %5.1f minute\" \n",
    "          % (i_chunk + 1, (time.time() - start_time) / 60))\n",
    "    # Free memory by deleting the current DataFrame\n",
    "    del df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also limit the access to the last N rows using the **skiprows** argument.\n",
    "\n",
    "Have a look at the first notebook to find out more on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating simple statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to show how you can calculate the probability a given IP address will be attributed.\n",
    "In other words we will try to compute :\n",
    "$$P\\left ( is\\_attributed= 1\\mid ip\\_address \\right )$$\n",
    "\n",
    "To do this we have to go though all ip addresses and sum up the number of times is_attributed=1 for each ip address and count the number of occurences of each ip address.\n",
    "\n",
    "The simplest way to do this is :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Read the data (here we limit to 10 million rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path, dtype=dtypes, nrows=10000000, usecols=['ip', 'is_attributed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory used for 10 million rows with only ip and is_attributed 47.68 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Memory used for 10 million rows with only ip and is_attributed %5.2f MB\"\n",
    "      % (df.memory_usage().sum() / 1024 ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Aggregate data by ip address using the groupby statement and using mean to calculate the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ip</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.015625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    is_attributed\n",
       "ip               \n",
       "9        0.000000\n",
       "10       0.000000\n",
       "19       0.000000\n",
       "20       0.000000\n",
       "25       0.015625\n",
       "27       0.000000\n",
       "31       0.000000\n",
       "32       0.000000\n",
       "36       0.000000\n",
       "39       0.033333"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_average = df.groupby(\"ip\").mean()\n",
    "ip_average.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 To create predictions for each ip we use the map statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000754\n",
       "1    0.000000\n",
       "2    0.000000\n",
       "3    0.001064\n",
       "4    0.000000\n",
       "5    0.000000\n",
       "6    0.000000\n",
       "7    0.000000\n",
       "8    0.000000\n",
       "9    0.007576\n",
       "Name: predictions, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"predictions\"] = df[\"ip\"].map(ip_average[\"is_attributed\"])\n",
    "df[\"predictions\"].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 We can now check the AUC score for this prediction using sklearn metric roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score for simple IP address predictions = 0.949700\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"AUC score for simple IP address predictions = %.6f\"\n",
    "      % (roc_auc_score(df[\"is_attributed\"], df[\"predictions\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this section you now know how to create predictions using the ip address only.\n",
    "\n",
    "However this is on the first 10 million rows only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Make predictions and calculate scores using the method above for the following features : \n",
    " - app\n",
    " - device\n",
    " - os\n",
    " - channel\n",
    " \n",
    "Please don't forget to free up memory after each prediction as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df, ip_average\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating statistics using the entire file\n",
    "\n",
    "In the previous method, reading the full file for ip and is_attributed would not be a problem and would take up to 890MB.\n",
    "\n",
    "However the groupby statement is very memory consuming and would certainly end up in a memory error situation.\n",
    "\n",
    "This is where using chunks is useful\n",
    "\n",
    "Please adapt chunksize to your own memory setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Chunks have been read in   0.2 minute\n",
      "  2 Chunks have been read in   0.4 minute\n",
      "  3 Chunks have been read in   0.6 minute\n",
      "  4 Chunks have been read in   0.8 minute\n",
      "  5 Chunks have been read in   1.1 minute\n",
      "  6 Chunks have been read in   1.3 minute\n",
      "  7 Chunks have been read in   1.5 minute\n",
      "  8 Chunks have been read in   1.7 minute\n",
      "  9 Chunks have been read in   2.0 minute\n",
      " 10 Chunks have been read in   2.1 minute\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ip</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1454.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.0</td>\n",
       "      <td>4029.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>846.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.0</td>\n",
       "      <td>5971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.0</td>\n",
       "      <td>446.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5007.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1039.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sum   count\n",
       "ip             \n",
       "1   9.0    47.0\n",
       "5   0.0    24.0\n",
       "6   2.0  1454.0\n",
       "9   6.0  4029.0\n",
       "10  3.0  1180.0\n",
       "19  4.0   846.0\n",
       "20  4.0  5971.0\n",
       "25  2.0   446.0\n",
       "27  8.0  5007.0\n",
       "31  3.0  1039.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ip_average as an empty DataFrame\n",
    "ip_average = None\n",
    "start_time=time.time()\n",
    "chunksize=20000000\n",
    "for i_chunk, df in enumerate(pd.read_csv(file_path, chunksize=chunksize, dtype=dtypes, usecols=['ip', 'is_attributed'])):\n",
    "    print(\"%3d Chunks have been read in %5.1f minute\" \n",
    "          % (i_chunk + 1, (time.time() - start_time) / 60))\n",
    "    # Make the groupby statement\n",
    "    # The groupby statement uses sum and count to be able to compute averages over all samples\n",
    "    the_group = df.groupby(\"ip\").agg(['sum', 'count'])\n",
    "    the_group.columns = the_group.columns.droplevel(0)\n",
    "    if ip_average is None:\n",
    "        ip_average = the_group\n",
    "    else:\n",
    "        # pandas .add method makes sure ip addresses that are not in both the_group and ip_average\n",
    "        # take value of 0 before the addition takes place\n",
    "        ip_average = the_group.add(ip_average, fill_value=0.0)\n",
    "\n",
    "    # Free memory by deleting the current DataFrame\n",
    "    del df, the_group\n",
    "    gc.collect()\n",
    "    \n",
    "ip_average.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the average per ip address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_average['average'] = ip_average['sum'] / ip_average['count'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to go through the file again to create the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Chunks have been read in   0.2 minute\n",
      "  2 Chunks have been read in   0.4 minute\n",
      "  3 Chunks have been read in   0.7 minute\n",
      "  4 Chunks have been read in   0.9 minute\n",
      "  5 Chunks have been read in   1.2 minute\n",
      "  6 Chunks have been read in   1.4 minute\n",
      "  7 Chunks have been read in   1.7 minute\n",
      "  8 Chunks have been read in   1.9 minute\n",
      "  9 Chunks have been read in   2.2 minute\n",
      " 10 Chunks have been read in   2.3 minute\n"
     ]
    }
   ],
   "source": [
    "# Create ip_average as an empty DataFrame\n",
    "start_time=time.time()\n",
    "# Create place holders for target and predictions to be able to compute the AUC score once the process has completed\n",
    "target = None\n",
    "predictions = None \n",
    "for i_chunk, df in enumerate(pd.read_csv(file_path, chunksize=chunksize, dtype=dtypes, usecols=['ip', 'is_attributed'])):\n",
    "    print(\"%3d Chunks have been read in %5.1f minute\" \n",
    "          % (i_chunk + 1, (time.time() - start_time) / 60))\n",
    "    if target is None:\n",
    "        target = df['is_attributed'].values\n",
    "        predictions = df['ip'].map(ip_average['average']).values\n",
    "    else:\n",
    "        target = np.hstack((target, df['is_attributed'].values))\n",
    "        predictions = np.hstack((predictions, df['ip'].map(ip_average['average']).values))\n",
    "        \n",
    "    # Free memory by deleting the current DataFrame\n",
    "    del df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display AUC score for this simple prediction on training dataset\n",
    "\n",
    "Please note this may take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score of predictions using ip on the whole dataset = 0.825532\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC score of predictions using ip on the whole dataset = %.6f\"\n",
    "      % (roc_auc_score(target, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Please use previous code to create predictions for :\n",
    " - app\n",
    " - device\n",
    " - channel\n",
    " - os\n",
    " \n",
    "And give us your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
