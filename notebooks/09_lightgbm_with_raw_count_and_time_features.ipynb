{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to demonstrate the use of the various features we have seen in the previous notebooks with a LightGBM classifier.\n",
    "\n",
    "At the end of this notebook, you will be able to :\n",
    " - Use all features we have created in our previous notebooks\n",
    " - Train a LightGBM classifier\n",
    " \n",
    "Hope you will enjoy this notebook and remember to come back to us and ask questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import time\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data (only 10 million rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define name and path of the training file\n",
    "file_path = \"../input/train.csv.zip\"\n",
    "# define column types\n",
    "dtypes = {\n",
    "    'ip': 'uint32',\n",
    "    'app': 'uint16',\n",
    "    'device': 'uint16',\n",
    "    'os': 'uint16',\n",
    "    'channel': 'uint16',\n",
    "    'is_attributed': 'uint8'\n",
    "}\n",
    "# Define columns we will use in the notebook\n",
    "cols=['ip', 'app', 'os', 'device', 'channel', 'click_time', 'is_attributed']\n",
    "# read the last rows of the dataset\n",
    "train_df = pd.read_csv(\n",
    "    file_path,\n",
    "    # nrows would select the first rows not the last rows so you need to use skiprows parameter\n",
    "    nrows=10000000, \n",
    "    # Use skiprows with the range function to access the last rows\n",
    "    # skiprows=range(1, 184903890 - 10000000),\n",
    "    dtype=dtypes, \n",
    "    usecols=cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Memory usage for train_df 0.196 GB'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check memory usage\n",
    "'Memory usage for train_df %5.3f GB' % (train_df.memory_usage().sum() / 1024 ** 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AverageManager class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a modified version of the AverageManager that simply returns counts instead of averages.\n",
    "\n",
    "The biggest issue with target averaging is overfitting since you use the target information nad this can be viewed as a leak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageManager(object):\n",
    "    \"\"\" Class that will manage target averages for selected feature and target encode these features \"\"\"\n",
    "    def __init__(self, features, target):\n",
    "        \"\"\" \n",
    "        Init an average manager for the given features and the specified target\n",
    "        :param features : expected to be a list of list of features to group by the data\n",
    "        :param target : name of the target feature\n",
    "        \"\"\"\n",
    "        # Check features : \n",
    "        for f_ in features:\n",
    "            if type(f_) != list:\n",
    "                raise ValueError('Features are expected to be provided as a list')\n",
    "        # averages contains the average data\n",
    "        self.averages = {\n",
    "            tuple(f_): None for f_ in features\n",
    "        }\n",
    "        # Prior contains the estimated prior of the target \n",
    "        self.prior = {'cum_sum': 0.0, 'nb_samples': 0.0}\n",
    "        # Conatins the name of the target column in the DataFrames\n",
    "        self.target = target\n",
    "        \n",
    "    def update_averages(self, df):\n",
    "        \"\"\"Update averages information using samples available in df\"\"\"\n",
    "        # update prior\n",
    "        self.prior['cum_sum'] += df[self.target].sum()\n",
    "        self.prior['nb_samples'] += df.shape[0]\n",
    "        \n",
    "        for f_ in self.averages.keys():\n",
    "            # Create the groupby\n",
    "            the_group = df[list(f_) + [self.target]].groupby(list(f_)).agg(['sum', 'count'])\n",
    "            the_group.columns = the_group.columns.droplevel(0)\n",
    "    \n",
    "            # Update the average\n",
    "            if self.averages[f_] is None:\n",
    "                self.averages[f_] = the_group\n",
    "            else:\n",
    "                # pandas .add method makes sure apps that are not in both the_group and current averages\n",
    "                # take value of 0 before the addition takes place\n",
    "                self.averages[f_] = the_group.add(self.averages[f_], fill_value=0.0)\n",
    "            \n",
    "            del the_group\n",
    "            gc.collect()\n",
    "            \n",
    "    def apply_averages(self, df):\n",
    "        \"\"\"Apply calculated averages on df to target encode the features\"\"\"\n",
    "        encoded = pd.DataFrame()\n",
    "        for f_ in self.averages.keys():\n",
    "            # Check averages are fitted\n",
    "            if self.averages[f_] is None:\n",
    "                raise ValueError('Averages have not been fitted yet')\n",
    "            # Compute the average\n",
    "            # self.averages[f_]['average'] = self.averages[f_]['sum'] / self.averages[f_]['count']\n",
    "            self.averages[f_]['average'] = self.averages[f_]['count']\n",
    "            # Now we need to encode for potetially several columns\n",
    "            feat_name = '_' + '_'.join(list(f_))\n",
    "            # Compute feataure on df\n",
    "            # df[feat_name] = df[list(f_)].apply(lambda row: '_'.join(row.astype(str)), axis=1, raw=True)\n",
    "            add_str_feature(df, list(f_), feat_name)\n",
    "            # Compute feature on the average\n",
    "            the_average = self.averages[f_].reset_index()\n",
    "            # the_average[feat_name] = the_average[list(f_)].apply(lambda row: '_'.join(row.astype(str)), axis=1, raw=True)\n",
    "            add_str_feature(the_average, list(f_), feat_name)\n",
    "            the_average.set_index(feat_name, inplace=True)\n",
    "            # finally map\n",
    "            encoded[feat_name] = df[feat_name].map(the_average['average']).astype(np.float32)\n",
    "            prior = self.prior['cum_sum'] / self.prior['nb_samples']\n",
    "            encoded[feat_name].fillna(prior, inplace=True)\n",
    "            # Drop feat_name from df\n",
    "            del df[feat_name]\n",
    "            gc.collect()\n",
    "        \n",
    "        return encoded\n",
    "        \n",
    "\n",
    "def add_str_feature(df_, features, name):\n",
    "    \"\"\"\n",
    "    It does the same as : \n",
    "    df[feat_name] = df[list(f_)].apply(lambda row: '_'.join(row.astype(str)), axis=1, raw=True)\n",
    "    However:\n",
    "     - The addition of series is faster than the apply statement \n",
    "     - apply(lambda x: str(x)) is faster than df_[f].astype(str)\n",
    "     \n",
    "    Without this function it would take 7.5 minutes to complete 3 chunks when it takes 3.5 minutes using it!\n",
    "    \"\"\"\n",
    "    df_[name] = ''\n",
    "    for f in features:\n",
    "        df_[name] += df_[f].apply(lambda x: str(x)) + '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_working_features():\n",
    "    return [\n",
    "        ['ip'], ['device'], ['channel'], ['os'], ['app'],\n",
    "        ['app', 'channel'], ['app', 'os'], ['os', 'channel'], ['ip', 'app']\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_target_feature():\n",
    "    return 'is_attributed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightGBM with raw features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0 AUC score : 0.950219\n",
      "Fold  1 AUC score : 0.953623\n",
      "Fold  2 AUC score : 0.949385\n",
      "Fold  3 AUC score : 0.949467\n",
      "Fold  4 AUC score : 0.948756\n",
      "LGBM with raw features OOF AUC 0.950156 AVG AUC 0.950290 +/- 0.001730\n"
     ]
    }
   ],
   "source": [
    "# Create folds\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "raw_features = ['ip', 'app', 'channel', 'os', 'device']\n",
    "# Test SGDClassifier on the data\n",
    "scores = []\n",
    "oof_predictions = np.zeros(train_df.shape[0])\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_df)):\n",
    "    clf = LGBMClassifier(\n",
    "        num_leaves=31,\n",
    "    )\n",
    "    clf.fit(train_df[raw_features].iloc[trn_idx].values, train_df[get_target_feature()].iloc[trn_idx].values)\n",
    "    oof_predictions[val_idx] = clf.predict_proba(train_df[raw_features].iloc[val_idx].values)[:,1]\n",
    "    scores.append(roc_auc_score(train_df[get_target_feature()].iloc[val_idx].values, \n",
    "                                oof_predictions[val_idx]))\n",
    "    print(\"Fold %2d AUC score : %.6f\" % (n_fold, scores[-1]))\n",
    "    del clf\n",
    "    gc.collect()\n",
    "    \n",
    "oof_score = roc_auc_score(train_df[get_target_feature()], oof_predictions)\n",
    "print(\"LGBM with raw features OOF AUC %.6f AVG AUC %.6f +/- %.6f\"\n",
    "      % (oof_score, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightGBM with raw features and count features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init average manager\n",
    "avg_man = AverageManager(features=get_working_features(), target=get_target_feature())\n",
    "# Fit average manager to the data\n",
    "avg_man.update_averages(train_df)\n",
    "# Target encode the data\n",
    "occurences = avg_man.apply_averages(train_df)\n",
    "# Add target encoding to train_df\n",
    "train_df = pd.concat([train_df, occurences], axis=1)\n",
    "\n",
    "del occurences, avg_man\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 AUC score : 0.964646\n",
      "Fold  2 AUC score : 0.967757\n",
      "Fold  3 AUC score : 0.966901\n",
      "Fold  4 AUC score : 0.965455\n",
      "Fold  5 AUC score : 0.962971\n",
      "LGBM with raw features OOF AUC 0.965370 AVG AUC 0.965546 +/- 0.001684\n"
     ]
    }
   ],
   "source": [
    "# Create folds\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "excluded_features = ['click_time', 'is_attributed', 'attributed_time']\n",
    "raw_features = [f_ for f_ in train_df if f_ not in excluded_features]\n",
    "\n",
    "# Test SGDClassifier on the data\n",
    "scores = []\n",
    "oof_predictions = np.zeros(train_df.shape[0])\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_df)):\n",
    "    clf = LGBMClassifier(\n",
    "        num_leaves=31,\n",
    "    )\n",
    "    clf.fit(train_df[raw_features].iloc[trn_idx].values, train_df[get_target_feature()].iloc[trn_idx].values)\n",
    "    oof_predictions[val_idx] = clf.predict_proba(train_df[raw_features].iloc[val_idx].values)[:,1]\n",
    "    scores.append(roc_auc_score(train_df[get_target_feature()].iloc[val_idx].values, \n",
    "                                oof_predictions[val_idx]))\n",
    "    print(\"Fold %2d AUC score : %.6f\" % (n_fold + 1, scores[-1]))\n",
    "    del clf\n",
    "    gc.collect()\n",
    "    \n",
    "oof_score = roc_auc_score(train_df[get_target_feature()], oof_predictions)\n",
    "print(\"LGBM with raw features OOF AUC %.6f AVG AUC %.6f +/- %.6f\"\n",
    "      % (oof_score, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightGBM with raw features, count features and time features\n",
    "\n",
    "#### Compute click_rate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert click_time to pd.DateTime\n",
    "train_df['click_time'] = pd.to_datetime(train_df['click_time'])\n",
    "train_df['day'] = train_df['click_time'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ip_app_day feature\n",
    "train_df['ip_app_day'] = train_df['ip'].apply(lambda x: str(x)) + '_' \\\n",
    "                         + train_df['app'].apply(lambda x: str(x)) + '_'\\\n",
    "                         + train_df['day'].apply(lambda x: str(x)) \n",
    "\n",
    "# Compute min, max click_time and counts for each ip and app\n",
    "ip_day_stats = train_df[['ip_app_day', 'click_time']].groupby('ip_app_day').agg(['min', 'max', 'count'])\n",
    "ip_day_stats.columns = ip_day_stats.columns.droplevel(0)\n",
    "\n",
    "# Convert min and max to integers\n",
    "ip_day_stats['max'] = ip_day_stats['max'].astype(np.int64) // 1e9\n",
    "ip_day_stats['min'] = ip_day_stats['min'].astype(np.int64) // 1e9\n",
    "# Compute the click rate\n",
    "ip_day_stats['click_rate_per_ip_app_day'] = (ip_day_stats['max'] - ip_day_stats['min']) / ip_day_stats['count'] \n",
    "# Map ip_app values with click_rate\n",
    "train_df['ip_app_day'] = train_df['ip_app_day'].map(ip_day_stats['click_rate_per_ip_app_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute time_difference feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert click time to integer\n",
    "train_df['click_time'] = (train_df['click_time'].astype(np.int64) // 10 ** 9).astype(np.int32)\n",
    "# Create time difference feature\n",
    "train_df['time_diff'] = (train_df.groupby(['ip', 'app', 'device', 'os']).click_time.shift(1)\\\n",
    "                         - train_df.click_time).astype(np.float32).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_df['click_time']\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 AUC score : 0.962632\n",
      "Fold  2 AUC score : 0.965644\n",
      "Fold  3 AUC score : 0.966927\n",
      "Fold  4 AUC score : 0.963998\n",
      "Fold  5 AUC score : 0.962223\n",
      "LGBM with raw features OOF AUC 0.962868 AVG AUC 0.964285 +/- 0.001783\n"
     ]
    }
   ],
   "source": [
    "# Create folds\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "excluded_features = ['click_time', 'is_attributed', 'attributed_time']\n",
    "raw_features = [f_ for f_ in train_df if f_ not in excluded_features]\n",
    "\n",
    "# Test SGDClassifier on the data\n",
    "scores = []\n",
    "oof_predictions = np.zeros(train_df.shape[0])\n",
    "for n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_df)):\n",
    "    clf = LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        num_leaves=31,\n",
    "        colsample_bytree=.8,\n",
    "        subsample=.8\n",
    "    )\n",
    "    clf.fit(\n",
    "        train_df[raw_features].iloc[trn_idx].values, \n",
    "        train_df[get_target_feature()].iloc[trn_idx].values,\n",
    "        eval_set=[(train_df[raw_features].iloc[val_idx].values, train_df[get_target_feature()].iloc[val_idx].values)],\n",
    "        eval_metric='auc', \n",
    "        verbose=False,\n",
    "        early_stopping_rounds=50\n",
    "    )\n",
    "    oof_predictions[val_idx] = clf.predict_proba(train_df[raw_features].iloc[val_idx].values)[:,1]\n",
    "    scores.append(roc_auc_score(train_df[get_target_feature()].iloc[val_idx].values, \n",
    "                                oof_predictions[val_idx]))\n",
    "    print(\"Fold %2d AUC score : %.6f\" % (n_fold + 1, scores[-1]))\n",
    "    del clf\n",
    "    gc.collect()\n",
    "    \n",
    "oof_score = roc_auc_score(train_df[get_target_feature()], oof_predictions)\n",
    "print(\"LGBM with raw features OOF AUC %.6f AVG AUC %.6f +/- %.6f\"\n",
    "      % (oof_score, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last Light GBM has worse performance than the previous one but you need to keep in mind we are just using the first 10 million rows, which is certainly not enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
