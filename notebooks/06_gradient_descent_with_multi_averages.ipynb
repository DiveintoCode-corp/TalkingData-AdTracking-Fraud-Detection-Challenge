{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to use previous learning materials to create predictions on the test dataset using an SGDClassifier and averages over multiple columns.\n",
    "\n",
    "In the previous notebook we used averages for **ip**, **app**, **channel**...\n",
    "\n",
    "In this notebook we will use averages for **app** AND **channel** as well.\n",
    "\n",
    "To cope with averages on multiple columns, **AverageManager** class has been modified. \n",
    "\n",
    "Please note that the python code in this notebook uses advanced pandas and python code. Therefore you may need time to really understand each line of code. \n",
    "\n",
    "PLEASE ask questions.\n",
    "\n",
    "At the end of this notebook you will know how to :\n",
    " - use SGDClassifier **partial_fit()** method to update a classifier with newly received samples\n",
    " - manage several target averages using a python **dict**\n",
    " - compute averages over multiple columns\n",
    " - create test predictions in a gzip format\n",
    " \n",
    "A public kaggle kernel has been created to help you submit the predictions to the LeaderBoard:\n",
    "\n",
    "https://www.kaggle.com/ogrellier/sgd-using-averages-of-previous-chunk\n",
    "\n",
    "This will make it simpler for you to fork the script and use it for your own experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import time\n",
    "import gc\n",
    "\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the file_path so that it points to where the train file is on your system  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../input/train.csv.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify data types to limit memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "        'ip': 'uint32',\n",
    "        'app': 'uint16',\n",
    "        'device': 'uint16',\n",
    "        'os': 'uint16',\n",
    "        'channel': 'uint16',\n",
    "        'is_attributed': 'uint8'\n",
    "    }\n",
    "cols = [f_ for f_ in dtypes.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a average manager class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageManager(object):\n",
    "    \"\"\" Class that will manage target averages for selected feature and target encode these features \"\"\"\n",
    "    def __init__(self, features, target):\n",
    "        \"\"\" \n",
    "        Init an average manager for the given features and the specified target\n",
    "        :param features : expected to be a list of list of features to group by the data\n",
    "        :param target : name of the target feature\n",
    "        \"\"\"\n",
    "        # Check features : \n",
    "        for f_ in features:\n",
    "            if type(f_) != list:\n",
    "                raise ValueError('Features are expected to be provided as a list')\n",
    "        # averages contains the average data\n",
    "        self.averages = {\n",
    "            tuple(f_): None for f_ in features\n",
    "        }\n",
    "        # Prior contains the estimated prior of the target \n",
    "        self.prior = {'cum_sum': 0.0, 'nb_samples': 0.0}\n",
    "        # Conatins the name of the target column in the DataFrames\n",
    "        self.target = target\n",
    "        \n",
    "    def update_averages(self, df):\n",
    "        \"\"\"Update averages information using samples available in df\"\"\"\n",
    "        # update prior\n",
    "        self.prior['cum_sum'] += df[self.target].sum()\n",
    "        self.prior['nb_samples'] += df.shape[0]\n",
    "        \n",
    "        for f_ in self.averages.keys():\n",
    "            # Create the groupby\n",
    "            the_group = df[list(f_) + [self.target]].groupby(list(f_)).agg(['sum', 'count'])\n",
    "            the_group.columns = the_group.columns.droplevel(0)\n",
    "    \n",
    "            # Update the average\n",
    "            if self.averages[f_] is None:\n",
    "                self.averages[f_] = the_group\n",
    "            else:\n",
    "                # pandas .add method makes sure apps that are not in both the_group and current averages\n",
    "                # take value of 0 before the addition takes place\n",
    "                self.averages[f_] = the_group.add(self.averages[f_], fill_value=0.0)\n",
    "            \n",
    "            del the_group\n",
    "            gc.collect()\n",
    "            \n",
    "    def apply_averages(self, df):\n",
    "        \"\"\"Apply calculated averages on df to target encode the features\"\"\"\n",
    "        encoded = pd.DataFrame()\n",
    "        for f_ in self.averages.keys():\n",
    "            # Check averages are fitted\n",
    "            if self.averages[f_] is None:\n",
    "                raise ValueError('Averages have not been fitted yet')\n",
    "            # Compute the average\n",
    "            self.averages[f_]['average'] = self.averages[f_]['sum'] / self.averages[f_]['count']\n",
    "            \n",
    "            # The next few lines of code are here to allow mapping averages to several columns\n",
    "            # Pandas does not allow to use the map statement on several columns so we need to group them into \n",
    "            # one single column.\n",
    "            # THIS IS THE TRICKY PART OF THIS CODE !\n",
    "            # Now we need to encode for potetially several columns\n",
    "            feat_name = '_' + '_'.join(list(f_))\n",
    "            # Now group the columns as a single string to make it only one \n",
    "            add_str_feature(df, list(f_), feat_name)\n",
    "            # Now group the average index columns as one string column and reindex using this string column\n",
    "            the_average = self.averages[f_].reset_index()\n",
    "            add_str_feature(the_average, list(f_), feat_name)\n",
    "            the_average.set_index(feat_name, inplace=True)\n",
    "            \n",
    "            # finally map on the single string column\n",
    "            encoded[feat_name] = df[feat_name].map(the_average['average']).astype(np.float32)\n",
    "            prior = self.prior['cum_sum'] / self.prior['nb_samples']\n",
    "            encoded[feat_name].fillna(prior, inplace=True)\n",
    "            # Drop feat_name from df\n",
    "            del df[feat_name]\n",
    "            gc.collect()\n",
    "        \n",
    "        return encoded\n",
    "        \n",
    "\n",
    "def add_str_feature(df_, features, name):\n",
    "    \"\"\"\n",
    "    It does the same as : \n",
    "    df[feat_name] = df[list(f_)].apply(lambda row: '_'.join(row.astype(str)), axis=1, raw=True)\n",
    "    However:\n",
    "     - The addition of series is faster than the apply statement \n",
    "     - apply(lambda x: str(x)) is faster than df_[f].astype(str)\n",
    "    \"\"\"\n",
    "    df_[name] = ''\n",
    "    for f in features:\n",
    "        df_[name] += df_[f].apply(lambda x: str(x)) + '_'\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read train dataset, update averages and train SGDClassifier\n",
    "\n",
    "Averages are updated on the fly and used to target encode the features one chunk after the other.\n",
    "\n",
    "SGDClassifier is trained using partial_fit.\n",
    "\n",
    "Doing things in this way will limit using future samples target on previous data like we would do when calating averages on the whole training set. Overall averages would use events that occured on the last day of the training set to encode events that occured on the 1st day of the training set.\n",
    "\n",
    "This is something we need to take care of when predicting on time series. \n",
    "\n",
    "Although I am not totally sure it is important in the TalkingData situation, I believe it would still limit overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk   1 scores : loss 0.011074 auc 0.953510 [  1.6 min used so far]\n",
      "Chunk   2 scores : loss 0.012969 auc 0.957749 [  3.1 min used so far]\n",
      "Chunk   3 scores : loss 0.012627 auc 0.969659 [  4.7 min used so far]\n",
      "Chunk   4 scores : loss 0.011466 auc 0.947955 [  6.2 min used so far]\n",
      "Chunk   5 scores : loss 0.013295 auc 0.962948 [  7.8 min used so far]\n",
      "Chunk   6 scores : loss 0.011556 auc 0.967542 [  9.4 min used so far]\n",
      "Chunk   7 scores : loss 0.011155 auc 0.945865 [ 10.9 min used so far]\n",
      "Chunk   8 scores : loss 0.013511 auc 0.949089 [ 12.5 min used so far]\n",
      "Chunk   9 scores : loss 0.013967 auc 0.957319 [ 14.1 min used so far]\n",
      "Chunk  10 scores : loss 0.010261 auc 0.972942 [ 14.5 min used so far]\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "# Create average manager\n",
    "used_features = ['app', 'os', 'channel']\n",
    "avg_features = [['app'], ['os'], ['app', 'channel']]\n",
    "avg_man = AverageManager(features=avg_features, target='is_attributed')\n",
    "\n",
    "# Init Classifier\n",
    "clf = SGDClassifier(loss='log', tol=1e-2)\n",
    "\n",
    "# Read train file \n",
    "chunksize=20000000\n",
    "for i_chunk, df in enumerate(pd.read_csv(file_path, \n",
    "                                         chunksize=chunksize, \n",
    "                                         dtype=dtypes, \n",
    "                                         usecols=used_features + ['is_attributed'])):\n",
    "    # Udpate averages with the average manager\n",
    "    avg_man.update_averages(df)\n",
    "    \n",
    "    # Apply averages usin the average manager\n",
    "    target_encoding = avg_man.apply_averages(df)\n",
    "    \n",
    "    # Update the SGDClassifier using current target encoding and calling partial_fit\n",
    "    clf.partial_fit(X=target_encoding, y=df['is_attributed'], classes=[0, 1])\n",
    "    \n",
    "    # Get current predictions\n",
    "    preds = clf.predict_proba(target_encoding)[:, 1]\n",
    "    \n",
    "    # Display the log_loss and AUC score on the current chunk\n",
    "    print(\"Chunk %3d scores : loss %.6f auc %.6f [%5.1f min used so far]\"\n",
    "          % (i_chunk + 1, \n",
    "             log_loss(df['is_attributed'], preds),\n",
    "             roc_auc_score(df['is_attributed'], preds),\n",
    "             (time.time() - start_time) / 60))\n",
    "    \n",
    "    del target_encoding\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 Chunks have been read in   0.3 minute\n",
      "  2 Chunks have been read in   0.6 minute\n",
      "  3 Chunks have been read in   0.9 minute\n",
      "  4 Chunks have been read in   1.2 minute\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "# Create place holder for the prediction\n",
    "predictions = None\n",
    "# PLEASE CHANGE THE TEST FILE PATH TO YOUR OWN SETTINGS\n",
    "test_file_path = '../input/test.csv.zip'\n",
    "chunksize = 5000000\n",
    "# Read the test file by chunks\n",
    "for i_chunk, df in enumerate(pd.read_csv(test_file_path, \n",
    "                                         chunksize=chunksize, \n",
    "                                         dtype=dtypes, \n",
    "                                         usecols=used_features + ['click_id'])):\n",
    "    if predictions is None:\n",
    "        # Get the click ids\n",
    "        # double square brackets are used to return a DataFrame and not a Series\n",
    "        predictions = df[['click_id']].copy() \n",
    "        # Encode df using average manager\n",
    "        target_encoding = avg_man.apply_averages(df)\n",
    "        # Predict probabilities with SGD Classifier\n",
    "        predictions['is_attributed'] = clf.predict_proba(target_encoding)[:, 1]\n",
    "    else:\n",
    "        # double square brackets are used to return a DataFrame and not a Series\n",
    "        curr_preds = df[['click_id']].copy() \n",
    "        # Encode df using average manager\n",
    "        target_encoding = avg_man.apply_averages(df)\n",
    "        # Predict probabilities with SGD Classifier\n",
    "        curr_preds['is_attributed'] = clf.predict_proba(target_encoding)[:, 1]\n",
    "        # Stack predictions and current predictions\n",
    "        predictions: pd.DataFrame = pd.concat([predictions, curr_preds], axis=0)\n",
    "        # free memory\n",
    "        del curr_preds\n",
    "        \n",
    "    # Free memory by deleting the current DataFrame\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Display the time we spent so far\n",
    "    print(\"%3d Chunks have been read in %5.1f minute\" \n",
    "          % (i_chunk + 1, (time.time() - start_time) / 60))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our predictions we need to store them in a file for submission.\n",
    "\n",
    "In this contest the submission file is quite big. To reduce its size, both for storage and submission over the web,we will use the following arguments:\n",
    " - float_format : it is used to cut the decimals of floats\n",
    " - compression : pandas can store files in a compressed format called gzip\n",
    " \n",
    "Writing this file can take some time! On my disk the file takes 108778KB.\n",
    "\n",
    "In the following statement:\n",
    " - **float_format** limits the number of decimal to 6\n",
    " - **compression** tells pandas to compress the file in gzip format\n",
    " - **index=False** tells pandas only to write the features themeselves in the file without the DataFrame index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('app_predictions.csv.gz', float_format='%.6f', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to avoid compressing the file or sending it through the web, which may also take some time, the best is to log into your Kaggle account and create kernel. You can then submit the result directly to the Leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Please use the previous code to create test predictions with more features and more feature combinations:\n",
    " - ip\n",
    " - device\n",
    " - channel\n",
    " - ip, app\n",
    " - app, device\n",
    " - app, os\n",
    " \n",
    "And give us your results and let us know if it helps you in progressing through the LeaderBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
